{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efc4166-279e-4a87-b0de-2aa3d193cdf6",
   "metadata": {},
   "source": [
    "#### CAPITULO 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a21d4d-ad1d-4a05-9d40-14f7ff5972e9",
   "metadata": {},
   "source": [
    "#### a. Descargamos el Quijote y probamos cositas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf951da3-2abf-4709-b65e-e1c63072357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions  import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "spark=(SparkSession\n",
    "       .builder\n",
    "       .appName(\"ProblemasSpark\")\n",
    "       .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d570e-e6b2-4c61-8c2b-c95fd38ac914",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote= spark.read.text(\"C:/ProblemasSpark/el_quijote.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26508530-0f66-4f50-81fd-be14ee6fbacd",
   "metadata": {},
   "source": [
    "Contamos el número de lineas que tiene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8e100-ecee-4472-bf32-e5a613f47a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f7c34-b1d7-4f56-9e19-2e46379c0a6a",
   "metadata": {},
   "source": [
    "Mostramos las primeras 20 lineas de código, para eso basta con usar .show():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19318f-d680-42da-b73c-c108d8bdfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333074f-00bf-4898-925e-3757a7c75d95",
   "metadata": {},
   "source": [
    "Podemos jugar con las opciones que ofrece el show, como mostrar el número que lineas que queramos y truncarlas si así lo deseamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05c70c-4648-4a9f-bd4e-0d8600b99a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35545673-2866-4a40-8fd3-8f9887a323dd",
   "metadata": {},
   "source": [
    "¿Cuales son las diferencias entre head,take y first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a382d47-e650-4bed-9cc3-7d783c68f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0bfa9-d9ce-468f-b4eb-b8435fbf48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6660b9bc-41b6-415a-84b0-8af3a83a6586",
   "metadata": {},
   "outputs": [],
   "source": [
    "quijote.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194e6b7f-6b1d-4a73-a2cd-b3e9101c0d1d",
   "metadata": {},
   "source": [
    "#### b. Ejercicio M&Ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86268be-46a8-413b-bf50-001815cfe5f1",
   "metadata": {},
   "source": [
    " i) Otras operaciones de agregación como el Max con otro tipo de ordenamiento (descendiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56565bc-363c-4db6-aed6-01bf3cc45f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df = (spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(\"C:/ProblemasSpark/mnm_dataset.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba047fc-84f1-47ea-9231-f9748b33b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc769f-dad6-4dd5-9c89-16da2b13cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9afe83e-1898-49d9-af3e-84410dccfed0",
   "metadata": {},
   "source": [
    "ii) Hacer un método con el where en el que pongas más de un estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0c538-77e2-4180-babd-4c537d6a3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mm_df = (mm_df\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .where((mm_df.State == \"CA\") | (mm_df.State == \"NV\"))\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(F.count(\"Count\").alias(\"Total\"))\n",
    " .orderBy(\"Color\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3d248-7c78-4109-bd74-c3dc6a4de02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mm_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274cfee-b4ac-4aec-9fba-80423cfabed9",
   "metadata": {},
   "source": [
    "iii)  Haz un ejercicio en el que se incluyan muchos estadisticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b527d42-c566-440a-991c-2c944ddaead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mm_df = (mm_df\n",
    " .select(\"State\", \"Color\", \"Count\")\n",
    " .where((mm_df.State == \"CA\") )\n",
    " .groupBy(\"State\", \"Color\")\n",
    " .agg(F.count(\"Count\").alias(\"Total\"),F.max(\"Count\").alias(\"maximo\"),F.min(\"Count\").alias(\"minimo\"),F.sum(\"Count\").alias(\"suma\"))\n",
    " .orderBy(\"Color\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c5a46-1e1a-4b19-bb0a-2e3cb92de40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mm_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e03dfb-6d50-412f-8575-983480960a6e",
   "metadata": {},
   "source": [
    "iv) Hacer también ejercicios en SQL creando tmpView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248454f-46fd-4d99-a677-31c6bc16fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_df.createTempView(\"caramelos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e33215-1c8f-49aa-b72c-c8691611d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM caramelos WHERE State='NV'   \").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3531d-cc68-4a91-8e9d-42e37bb7a346",
   "metadata": {},
   "source": [
    "## CAPITULO 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf6eb79-b26e-4110-8a83-187b693c2500",
   "metadata": {},
   "source": [
    "#### Cuando se define un schema al definir un campo por ejemplo StructField('Delay', FloatType(), True) ¿qué significa el último parámetro Boolean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3b7e7-c41a-4bd6-bebd-9d8244b70094",
   "metadata": {},
   "source": [
    "El booleano indica si el campo puede ser nulo (None) o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c946b9-ea54-4f53-abbb-278ece9a4598",
   "metadata": {},
   "source": [
    "####  Dataset vs DataFrame vs RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616d0cf-586e-4fb0-b178-2599e972ea23",
   "metadata": {},
   "source": [
    "RDD, DataFrame y Dataset son las tres estructuras de datos más comunes en Spark y hacen que el procesamiento de datos muy grandes sea fácil y conveniente. Debido al algoritmo de evaluación perezoso de Spark, estas estructuras de datos no se ejecutan directamente durante las creaciones, transformaciones y funciones, etc. Solo cuando encuentran acciones, inician la operación transversal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2fc4a0-2a35-4527-8bf5-4feb8ea8d226",
   "metadata": {},
   "source": [
    "**RDD** significa conjunto de datos Distribuido Resistente. Es una colección de particiones inmutables grabadas. RDD es la estructura de datos fundamental de Spark, cuyas particiones se barajan, se envían a través de nodos y se operan en paralelo. Permite a los programadores realizar análisis complejos en memoria en grandes clústeres con tolerancia a fallas. RDD puede manejar datos estructurados y no estructurados de manera fácil y efectiva, ya que tiene muchos operadores funcionales integrados como group, map y filter, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d952fe-77cc-4b5f-a011-18b3e05f1db4",
   "metadata": {},
   "source": [
    "Sin embargo, cuando se encuentra con una lógica compleja, **RDD** tiene una desventaja muy obvia: los operadores no se pueden reutilizar. Esto se debe a que RDD no conoce la información de los datos almacenados, por lo que la estructura de los datos es una caja negra que requiere que el usuario escriba una función de agregación muy específica para completar una ejecución. Por lo tanto, **RDD es preferible en datos no estructurados**, para usarse para transformaciones y acciones de bajo nivel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea1ce1-da65-48df-a3f5-e6b69299d4ca",
   "metadata": {},
   "source": [
    "**DataFrame** es un conjunto de datos distribuido basado en RDD que organiza los datos en columnas con nombre antes de Spark 2.0. Es similar a una tabla bidimensional en la base de datos relacional, por lo que presenta el esquema de la base de datos. Por eso, **puede tratarse como una optimización sobre los RDD**; por ejemplo, los DAta\n",
    "frames conocen la estructura de los datos almacenados, lo que permite a los usuarios realizar operaciones de alto nivel. Con respecto a eso, maneja datos estructurados y semiestructurados. Los usuarios pueden ser específicos en qué columna realizar qué operaciones. Esto permite que un operador se use en múltiples columnas y hace que un operador sea reutilizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c8932-2af4-4969-9fb0-e9b7fd45f28d",
   "metadata": {},
   "source": [
    "**DataFrame** también hace que las operaciones de renovación sean más fáciles y flexibles. Si los usuarios tienen requisitos adicionales en la operación existente que necesita incluir otra columna en la operación, los usuarios pueden simplemente escribir un operador para esa columna adicional y agregarlo a la operación existente. Sin embargo, RDD necesita hacer muchos cambios en la agregación existente. En comparación con RDD, DataFrame no proporciona seguridad de tipos en tiempo de compilación, ya que es una colección distribuida de objetos Row. Al igual que RDD, DataFrame también admite varias API. A diferencia de RDD, DataFrame se puede usar con Spark SQL como la estructura de datos que almacena, por lo que puede proporcionar operadores más funcionales y permitir a los usuarios realizar operaciones basadas en expresiones y UDF. Por último, pero no menos importante, puede mejorar la eficiencia de ejecución, reducir el costo de cargar los datos y optimizar los planes lógicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193d27f-0a91-4588-9bdb-5d3c0bfad6ce",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset** API es como una extensión y mejora de DataFrame API. Externamente, Dataset es una colección de objetos JVM. Internamente, Dataset tiene una vista sin tipo llamada DataFrame, que es un Dataset of Row desde Spark 2.0. Dataset combina las ventajas de RDD y DataFrame. Al igual que RDD, admite el almacenamiento de datos estructurados, no estructurados y personalizados, y brinda a los usuarios un estilo de programación orientado a objetos y seguridad de tipos en tiempo de compilación. Al igual que DataFrame, aprovecha el optimizador Catalyst para permitir a los usuarios realizar consultas SQL estructuradas sobre los datos, pero es más lento que DataFrame. A diferencia de RDD y DataFrame, solo es compatible con las API de Java y Scala. Las API para Python y R todavía están en desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ef7f0-a2d3-4777-a09b-c4f2d6ceafd9",
   "metadata": {},
   "source": [
    "#### Realizando los ejercicios propuestos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf999eef-e8d9-489b-9ac3-da293672a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la localización del dataset\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "sf_fire_file=\"C:/ProblemasSpark/sf-fire-calls.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b09e9d-2105-4e98-af86-d2d030c2f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos nuestro esquema ya que el archivo tiene 4 millones de registros. Es costoso inferir el esquema para archivos tan grandes.\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    " StructField('UnitID', StringType(), True),\n",
    " StructField('IncidentNumber', IntegerType(), True),\n",
    " StructField('CallType', StringType(), True), \n",
    " StructField('CallDate', StringType(), True), \n",
    " StructField('WatchDate', StringType(), True),\n",
    " StructField('CallFinalDisposition', StringType(), True),\n",
    " StructField('AvailableDtTm', StringType(), True),\n",
    " StructField('Address', StringType(), True), \n",
    " StructField('City', StringType(), True), \n",
    " StructField('Zipcode', IntegerType(), True), \n",
    " StructField('Battalion', StringType(), True), \n",
    " StructField('StationArea', StringType(), True), \n",
    " StructField('Box', StringType(), True), \n",
    " StructField('OriginalPriority', StringType(), True), \n",
    " StructField('Priority', StringType(), True), \n",
    " StructField('FinalPriority', IntegerType(), True), \n",
    " StructField('ALSUnit', BooleanType(), True), \n",
    " StructField('CallTypeGroup', StringType(), True),\n",
    " StructField('NumAlarms', IntegerType(), True),\n",
    " StructField('UnitType', StringType(), True),\n",
    " StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    " StructField('FirePreventionDistrict', StringType(), True),\n",
    " StructField('SupervisorDistrict', StringType(), True),\n",
    " StructField('Neighborhood', StringType(), True),\n",
    " StructField('Location', StringType(), True),\n",
    " StructField('RowID', StringType(), True),\n",
    " StructField('Delay', FloatType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8dc8b-0f02-4826-b3cf-1d0bc9b367e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58328e0-0c52-4ab9-a712-97073e722ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cacheamos el DF ya que le ejecutaremos diversas operaciones\n",
    "fire_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b10e8b-d105-4cb4-8422-c4706017381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5377ad3-d1a8-4425-967f-cd0432f6286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939efe54-135c-4530-8c54-3c25d5dc2d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_df.select(\"CallDate\",\"WatchDate\",\"AvailableDtTm\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43b323-b182-4916-b7c4-734d60f5cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtramos las llamadas de tipo \"incidente médico\"\n",
    "few_fire_df=(fire_df\n",
    "             .select(\"IncidentNumber\",\"AvailableDtTm\",\"CallType\")\n",
    "             .where(col(\"CallType\")==\"Medical Incident\"))\n",
    "few_fire_df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f466daf-a9f6-4861-b0fa-ae61da206819",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cuántos tipos distintos de llamadas se hicieron al Departamento de Bomberos?\n",
    "# Como es habitual, nos cercioramos de no contabilizar strings nulos.\n",
    "fire_df.select(\"CallType\").where(col(\"CallType\").isNotNull()).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1ef21-7d77-4053-b675-f46b145c7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostremos algunas de esas llamadas\n",
    "fire_df.select(\"CallType\").where(col(\"CallType\").isNotNull()).distinct().show(15,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4bb1d-f7de-40dd-82bd-bb043c63d7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descubre todos los tiempos de respuesta o de retraso superiores a 5 minutos\n",
    "\n",
    "#1.Cambie el nombre de la columna Delay -> ReponseDelayedinMins\n",
    "#2.Devuelve un nuevo DataFrame\n",
    "#3.Averigüe todas las llamadas en las que el tiempo de respuesta al lugar del incendio se retrasó más de 5 minutos\n",
    "\n",
    "new_fire_df=fire_df.withColumnRenamed(\"Delay\",\"ResponseDelayedinMins\")\n",
    "new_fire_df.select(\"CallNumber\",\"ResponseDelayedinMins\").where(col(\"ResponseDelayedinMins\")>5).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2f1d5-cfb5-4e93-8dd2-d28f4616a4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hagamos algo de ETL:\n",
    "\n",
    "#1.Transforme las fechas de la cadena al tipo de datos Spark Timestamp para que podamos realizar algunas consultas basadas en el tiempo más adelante\n",
    "#2.Devuelve una consulta transformada\n",
    "#.Almacenar en caché el nuevo DataFrame\n",
    "\n",
    "fire_ts_df = (new_fire_df\n",
    "              .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\") \n",
    "              .withColumn(\"OnWatchDate\",   to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
    "              .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\")).drop(\"AvailableDtTm\"))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0e2b9-1047-4741-96ea-24a362298084",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_ts_df.cache()\n",
    "fire_ts_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f59cf-2dd2-432e-bb40-c0461b6dc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos las columnas transformadas con el tipo Spark Timestamp\n",
    "fire_ts_df.select(\"IncidentDate\",\"OnWatchDate\",\"AvailableDtTS\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f7eb2-e289-41d0-b0e8-0ac32ecd46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cuáles fueron los tipos de llamadas más comunes? Listémonos en orden decreciente.\n",
    "(fire_ts_df.select(\"CallType\")\n",
    "           .where(col(\"CallType\").isNotNull())\n",
    "           .groupBy(\"CallType\")\n",
    "           .count()\n",
    "           .orderBy(\"count\",ascending=False)\n",
    "           .show(10,False)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61cd3f-d2ff-42c1-8a37-dc422b3d4849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Qué códigos postales representaron las llamadas más comunes?\n",
    "\n",
    "#Investiguemos qué códigos postales en San Francisco representaron la mayoría de las llamadas de bomberos y de qué tipo fueron.\n",
    "\n",
    "#1.Filtrar por tipo de llamada\n",
    "#2.Agruparlos por tipo de llamada y código postal\n",
    "#3.Cuéntalos y muéstralos en orden descendente.\n",
    "\n",
    "\n",
    "(fire_ts_df.select(\"CallType\",\"ZipCode\")\n",
    "           .where(col(\"CallType\").isNotNull())\n",
    "           .groupBy(\"CallType\",\"ZipCode\")\n",
    "           .count()\n",
    "           .orderBy(\"count\",ascending=False)\n",
    "           .show(10,False))\n",
    "\n",
    "#Parece que las llamadas más comunes estaban relacionadas con incidentes médicos y los dos códigos postales son 94102 y 94103."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d7e56d-2646-41b6-898e-ce7f06c7c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cuáles son los vecindarios de San Francisco en los códigos postales 94102 y 94103?\n",
    "# Es probable que sean aquellos con una tasa de delicuencia mayor\n",
    "(fire_ts_df.select(\"Neighborhood\",\"Zipcode\")\n",
    "           .where((col(\"Zipcode\")==94102)|(col(\"Zipcode\")==94103))\n",
    "           .distinct()\n",
    "           .show(10,truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134a6d8-dfc6-4de3-97bc-b901d3e59190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cuál fue la suma de todas las llamadas, promedio, mínimo y máximo de los tiempos de respuesta de las llamadas?\n",
    "\n",
    "#Usemos las funciones integradas de Spark SQL para calcular la suma, el promedio, el mínimo y el máximo de algunas columnas:\n",
    "\n",
    "#1.Número de alarmas totales\n",
    "#2.¿Cuál fue el mínimo y el máximo de demora en el tiempo de respuesta antes de que el Departamento de Bomberos llegara al lugar de la llamada?\n",
    "\n",
    "fire_ts_df.select(sum(\"NumAlarms\"),avg(\"ResponseDelayedinMins\"),min(\"ResponseDelayedinMins\"),max(\"ResponseDelayedinMins\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa87dd-1214-46c2-a1dd-d185f79d3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cuántos años distintos de datos hay en el archivo CSV?\n",
    "\n",
    "#Podemos usar la función year() SQL Spark fuera del tipo de datos de la columna Timestamp IncidentDate.\n",
    "\n",
    "#En total, tenemos llamadas de incendio de los años 2000-2018\n",
    "\n",
    "fire_ts_df.select(year(\"IncidentDate\")).distinct().orderBy(year(\"IncidentDate\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc79a24-d7d2-4365-91d8-aa4a767a1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Qué semana del año en 2018 tuvo la mayor cantidad de llamadas de emergencia?\n",
    "fire_ts_df.filter(year(\"IncidentDate\")==2018).groupBy(weekofyear(\"IncidentDate\")).count().orderBy(\"count\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c75606-b497-46c8-8cc3-480b7c9e5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Qué vecindarios en San Francisco tuvieron el peor tiempo de respuesta en 2018?\n",
    "\n",
    "fire_ts_df.select(\"Neighborhood\",\"ResponseDelayedinMins\").filter(year(\"IncidentDate\")==2018).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e16503-3d3b-4194-a2b8-0c89a10602e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¿Cómo podemos usar archivos Parquet o tablas SQL para almacenar datos y leerlos?\n",
    "fire_ts_df.write.format(\"parquet\").mode(\"overwrite\").save(\"C:/Users/josemanuel.moya/Desktop/Videoclases/SPARK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca88a01-29d5-455f-ae83-66328ce819b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_ts_df.write.format(\"JSON\").mode(\"overwrite\").save(\"C:/Users/josemanuel.moya/Desktop/Videoclases/SPARK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194cd7f-5967-4e54-b219-f9e2ccfebd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_ts_df.write.format(\"csv\").mode(\"overwrite\").save(\"C:/Users/josemanuel.moya/Desktop/Videoclases/SPARK/contrato.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1b0b4-37c1-484b-ba65-1b2b509dc3ec",
   "metadata": {},
   "source": [
    "#### f. Revisar al guardar los ficheros (p.e. json, csv, etc) el número de ficheros creados, revisar su contenido para comprender (constatar) como se guardan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2064079-65bc-4488-8024-086b9783cf49",
   "metadata": {},
   "source": [
    "## CAPITULO 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b887b-c77c-4f55-9a01-e43efbb3300d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### a) Realizar los ejercicios propuestos de este capítulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ea464-01d7-4f77-9e67-04bd490e2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos un UDF para convertir la fecha a un formato legible\n",
    "\n",
    "def to_date_format_udf(d_str):\n",
    "  l = [char for char in d_str]\n",
    "  return \"\".join(l[0:2]) + \"/\" +  \"\".join(l[2:4]) + \" \" + \" \" +\"\".join(l[4:6]) + \":\" + \"\".join(l[6:])\n",
    "\n",
    "to_date_format_udf(\"26102000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760af92f-023a-4dc7-87b0-53814bdc97fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registramos el UDF\n",
    "\n",
    "spark.udf.register(\"to_date_format_udf\",to_date_format_udf,StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67e794-ffc5-4db3-aafc-9f255fb44126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leemos nuestro dataset departuredelays.csv\n",
    "\n",
    "df = (spark.read.format(\"csv\")\n",
    "      .schema(\"date STRING, delay INT, distance INT, origin STRING, destination STRING\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"path\", \"C:/ProblemasSpark/departuredelays.csv\")\n",
    "      .load())\n",
    "\n",
    "df.select(\"date\",\"delay\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1255f-998f-41df-8135-07e60651b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testeamos nuestro udf\n",
    "spark.sql(\"SELECT date,to_date_format_udf(date) as PrimerUDF FROM table\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdab91e-8a91-402c-bb32-559f134bf14b",
   "metadata": {},
   "source": [
    "#### b) GlobalTempView vs TempView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bfd7ab-a54b-4028-a103-43ff21f0aea8",
   "metadata": {},
   "source": [
    "La diferencia radica en el tiempo de vida de cada una. Mientras que esta última está atada a la SparkSession, la primera solo depende de la propia aplicación de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260afc4f-75f9-425c-a6e7-59cb8ca04559",
   "metadata": {},
   "source": [
    "#### c) Leer los AVRO,Parquet, JSON y CSV escritos en el cap3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a15d9f-b20d-4a32-8c2e-ef9ec920dfe8",
   "metadata": {},
   "source": [
    "## CAPITULO 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb20fb-6cf3-494c-aa28-a20395747383",
   "metadata": {},
   "source": [
    "#### a) Realizar todos los ejercicios propuestos de libro (excepto los de hive en caso de utilizar spark instalado en local y en el caso de RDBMS hacer únicamente ejemplo especificado más adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4039254-e017-49f7-a59e-1152cf617e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si bien Apache Spark tiene una gran cantidad de funciones, la flexibilidad de Spark permite que los ingenieros \n",
    "#y científicos de datos definan sus propias funciones (es decir, funciones definidas por el usuario o UDF).\n",
    "\n",
    "#Creamos la función elevar al cubo\n",
    "def cubed(s):\n",
    "    return s*s*s\n",
    "\n",
    "#Registramos el UDF\n",
    "\n",
    "spark.udf.register(\"cubed\",cubed,LongType())\n",
    "\n",
    "#Generamos una vista temporal\n",
    "\n",
    "spark.range(1,9).createOrReplaceTempView(\"udf_test\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00629290-3e87-4d78-9cf2-75c91c63cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT id,cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de114279-b917-4261-bca8-be3c5d39a3d5",
   "metadata": {},
   "source": [
    "**Aceleración y distribución de PySpark UDF con Pandas UDF**\n",
    "\n",
    "Uno de los problemas predominantes anteriores con el uso de las UDF de PySpark era que tenía un rendimiento más lento que las UDF de Scala. Esto se debió a que las UDF de PySpark requerían el movimiento de datos entre el funcionamiento de JVM y Python, lo cual era bastante costoso. Para resolver este problema, se introdujeron las UDF pandas (también conocidas como UDF vectorizadas) como parte de Apache Spark 2.3. Es un UDF que usa Apache Arrow para transferir datos y utiliza pandas para trabajar con los datos. Simplifica la definición de una UDF de pandas utilizando la palabra clave pandas_udf como decorador o para envolver la función en sí. Una vez que los datos están en formato Apache Arrow, ya no es necesario serializar/decapar los datos, ya que ya están en un formato consumible por el proceso de Python. En lugar de operar en entradas individuales fila por fila, está operando en una serie de pandas o marco de datos (es decir, ejecución vectorizada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56449a22-8aee-4b01-9709-bd53cb367115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaramos la función elevar al cubo\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "#Creamos la UDF pandas para la función elevar al cubo\n",
    "\n",
    "cubed_udf=pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3960d3-e5a0-49a3-971b-4b9fbc6867b8",
   "metadata": {},
   "source": [
    "#### Usamos pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e97f6-b7f9-4d82-bc52-03f2a04b9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una pandas series\n",
    "x=pd.Series([1,2,3])\n",
    "\n",
    "\n",
    "#La función para pandas_udf ejecutada con datos locales de Pandas\n",
    "\n",
    "print(cubed(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e97ed-91ce-4bb9-bfad-a59e3256c90d",
   "metadata": {},
   "source": [
    "#### Usamos Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0b393-4797-4934-a202-eb4dc411a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un dataFrame de Spark\n",
    "\n",
    "df=spark.range(1,4)\n",
    "\n",
    "#Ejecutamos función como un UDF vectorizado de Spark\n",
    "\n",
    "df.select(\"id\",cubed_udf(col(\"id\"))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558dd8d6-bc0d-44a9-b527-614a8c53a2f9",
   "metadata": {},
   "source": [
    "#### Funciones de orden superior en DataFrames y Spark SQL\n",
    "Debido a que los tipos de datos complejos son una amalgama de tipos de datos simples, es tentador manipular tipos de datos complejos directamente. Como se señaló en la publicación Introducción de nuevas funciones integradas y de orden superior para tipos de datos complejos en Apache Spark 2.4, normalmente hay dos soluciones para la manipulación de tipos de datos complejos.\n",
    "\n",
    "-Explosión de la estructura anidada en filas individuales, aplicación de alguna función y luego recreación de la estructura anidada como se indica en el fragmento de código a continuación.\n",
    "\n",
    "-Creación de una función definida por el usuario (UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ae1f2-c6b9-47ea-b209-f5901c2e9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array dataset\n",
    "arrayData = [[1, (1, 2, 3)], [2, (2, 3, 4)], [3, (3, 4, 5)]]\n",
    "\n",
    "# Create schema\n",
    "from pyspark.sql.types import *\n",
    "arraySchema = (StructType([\n",
    "      StructField(\"id\", IntegerType(), True), \n",
    "      StructField(\"values\", ArrayType(IntegerType()), True)\n",
    "      ]))\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(spark.sparkContext.parallelize(arrayData), arraySchema)\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880501d0-8884-4ced-80e4-42d109a42ea9",
   "metadata": {},
   "source": [
    "**OPCION 1: EXPLODE Y COLLECT**\n",
    "\n",
    "En esta instrucción SQL anidada, primero hacemos `explode(values)` que crea una nueva fila (con la identificación) para cada elemento (`VALUE`) dentro de los valores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b987b6d-cabc-4c75-893e-8f65f1c7ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT id, collect_list(value + 1) AS newValues\n",
    "  FROM  (SELECT id, explode(values) AS value\n",
    "        FROM table) x\n",
    " GROUP BY id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28771ed1-cf0d-46ad-a7a1-fd534dbe8115",
   "metadata": {},
   "source": [
    "#### b) ¿En qué consisten las UDFs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77926a7d-fb2a-4970-95ed-fbc01431837a",
   "metadata": {},
   "source": [
    "**UDF** (User Defined Functions) son las funciones de usuario, y son sistemas para definir nuevos métodos SQL que operan sobre las columnas de un DataFrame.\n",
    "Spark SQL ya tiene operaciones sobre columnas, como filtrar valores en un rango, pero podemos utilizar las UDFs para definir la lógica de nuestro negocio.\n",
    "Para definir una UDF en Scala necesitamos:\n",
    "- Importar org.apache.spark.sql.functions.\n",
    "- Para definir una UDF tenemos un método que se llama “udf”, que recibe como argumento una lista de entradas, el carácter clave igual y mayor que, para separarlos de la lógica de la función, y después ponemos una lógica con la secuencia de operaciones hasta el retorno deseado: val miUdf = udf((x1: Tipo1, x2: Tipo2, …. , xN: TipoN) => … lógica de la función…).\n",
    "\n",
    "Una vez que hemos definido una UDF lo que hay que hacer es generar una columna en nuestro DataFrame, con el resultado de aplicar esa UDF.\n",
    "\n",
    "\n",
    "Para ello, se utiliza withColumn, que se le pasa como primer parámetro el nombre de la nueva columna general, y como segundo parámetro el nombre de la UDF y la serie de columnas sobre las que se aplica:\n",
    "\n",
    "miDataFrame.withColumn(“NuevaCol”, miUDF(col(“c1”), …, col(“cN”)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b603353-12e1-45f6-8868-b28bcec0318c",
   "metadata": {},
   "source": [
    "#### c) Instalar MySQL, descargar driver y cargar datos de BBDD de empleados "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36a38d-358a-4943-9df5-babc42417210",
   "metadata": {},
   "source": [
    " ##### c.1) Cargar con spark datos de empleados y departamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7165f2d-c7b3-4dc3-85ba-f4d778b873a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"employees\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"JXP3Yzqe\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519c792-65d7-40f2-8738-4d85dbbe966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "depar = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"departments\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"JXP3Yzqe\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554bd800-d0c3-415e-b5b9-173b2b1fd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "depman = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"dept_manager\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"JXP3Yzqe\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6f9cb-cd64-4254-9443-380978bd67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "depem = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"dept_emp\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"JXP3Yzqe\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210034c7-6770-4887-9410-08fadb604600",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"titles\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"JXP3Yzqe\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6c5fc-7453-48e2-a78f-cced121c87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries = (spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    " .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    " .option(\"dbtable\", \"salaries\")\n",
    " .option(\"user\", \"root\")\n",
    " .option(\"password\", \"JXP3Yzqe\")\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc573277-12df-4668-8846-fe7568d2bc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae0841-ba76-435c-a950-272cdc0dea80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e024dc6-5947-4122-aef6-a5cdb1d66228",
   "metadata": {},
   "source": [
    "##### c.2) Mediante Joins mostrar toda la información de los empleados además de su título y salario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19dd56c-c76a-4a0f-a4fc-98fda5cfce89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "employees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb92ba0-7305-4d81-9c41-a6785e8c7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "depar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cab0b3-0d12-4514-ab7d-9f5806b41a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "depman.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8338277-3cba-4881-915a-f98c46e480c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "depem.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921cdc57-f95a-4b84-aa45-1c7dfea07ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a4e4b-dd28-4d02-a988-a1be2b0fec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8718140-f5a2-4ff1-a113-ceb80603581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT employees.birth_date,employees.first_name,employees.last_name,titles.title,salaries.salary\\\n",
    "           FROM employees\\\n",
    "           JOIN titles ON employees.emp_no= titles.emp_no\\\n",
    "           JOIN salaries ON employees.emp_no= salaries.emp_no\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f54e2-656c-4571-9cdc-dc2c40a861c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM salaries\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc0808-015f-4cda-a424-b8c064af2954",
   "metadata": {},
   "source": [
    "##### c.3) Diferencia entre Rank y dense_rank (operaciones de ventana) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee995c24-e4bb-4933-a4b8-652095407e65",
   "metadata": {},
   "source": [
    "##### c.4) Utilizando operaciones de ventana obtener el salario, posición (cargo) y departamento actual de cada empleado, es decir, el último o más reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2941d-d63f-4508-b79f-176fc367f47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
