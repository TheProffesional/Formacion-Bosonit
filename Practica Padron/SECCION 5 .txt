5- Trabajando con tablas en HDFS.
 A continuación vamos a hacer una inspección de las tablas, tanto externas (no 
gestionadas) como internas (gestionadas). Este apartado se hará si se tiene acceso y conocimiento 
previo sobre cómo insertar datos en HDFS.




• 5.3)Crear un directorio en HDFS con un nombre a placer, por ejemplo, /test. Si estás en 
una máquina Cloudera tienes que asegurarte de que el servicio HDFS está activo ya 
que puede no iniciarse al encender la máquina (puedes hacerlo desde el Cloudera 
Manager). A su vez, en las máquinas Cloudera es posible (dependiendo de si 
usamos Hive desde consola o desde Hue) que no tengamos permisos para crear 
directorios en HDFS salvo en el directorio /user/cloudera.


hdfs dfs -mkdir /user/cloudera/test


• 5.4)Mueve tu fichero datos1 al directorio que has creado en HDFS con un comando 
desde consola.


hdfs dfs -put /home/cloudera/Desktop/SECCION5/datos1 /user/cloudera/test


• 5.5)Desde Hive, crea una nueva database por ejemplo con el nombre numeros. Crea 
una tabla que no sea externa y sin argumento location con tres columnas 
numéricas, campos separados por coma y delimitada por filas. La llamaremos por 
ejemplo numeros_tbl.

create database numeros;

use numeros;

create table sinaloa(
columna1 INT,
columna2 INT,
columna3 INT

)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE


• 5.6)Carga los datos de nuestro fichero de texto datos1 almacenado en HDFS en la tabla 
de Hive. Consulta la localización donde estaban anteriormente los datos 
almacenados. ¿Siguen estando ahí? ¿Dónde están?. Borra la tabla, ¿qué ocurre con 
los datos almacenados en HDFS?


Vemos si siguen en la ubicación:

hdfs dfs -ls /user/cloudera/test


Los datos ya no están ahí.





• 5.7)Vuelve a mover el fichero de texto datos1 desde el almacenamiento local al 
directorio anterior en HDFS.


hdfs dfs -put /home/cloudera/Desktop/SECCION5/datos1 /user/cloudera/test


• 5.8)Desde Hive, crea una tabla externa sin el argumento location. Y carga datos1 (desde 
HDFS) en ella. ¿A dónde han ido los datos en HDFS? Borra la tabla ¿Qué ocurre con 
los datos en hdfs?

create external table sinaloa(
columna1 INT,
columna2 INT,
columna3 INT

)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE


LOAD DATA INPATH '/user/cloudera/test/datos1' INTO TABLE sinaloa;

Vemos si siguen en la ubicación:

hdfs dfs -ls /user/cloudera/test

Los datos han desaparecido y ahora estan en esta ubicación:

hdfs dfs -ls /user/hive/warehouse/numeros.db

• 5.9) Borra el fichero datos1 del directorio en el que estén. Vuelve a insertarlos en el 
directorio que creamos inicialmente (/test). Vuelve a crear la tabla numeros desde 
hive pero ahora de manera externa y con un argumento location que haga 
referencia al directorio donde los hayas situado en HDFS (/test). No cargues los 
datos de ninguna manera explícita. Haz una consulta sobre la tabla que acabamos 
de crear que muestre todos los registros. ¿Tiene algún contenido?


hdfs dfs -rm /user/hive/warehouse/numeros.db/sinaloa/datos1


hdfs dfs -put /home/cloudera/Desktop/SECCION5/datos1 /user/cloudera/test


create table sinaloa(
columna1 INT,
columna2 INT,
columna3 INT

)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/cloudera/test/';


SELECT * FROM sinaloa;

• 5.10)Inserta el fichero de datos creado al principio, "datos2" en el mismo directorio de 
HDFS que "datos1". Vuelve a hacer la consulta anterior sobre la misma tabla. ¿Qué 
salida muestra? 


hdfs dfs -put /home/cloudera/Desktop/SECCION5/datos2 /user/cloudera/test

SELECT * FROM sinaloa


• 5.11)Extrae conclusiones de todos estos anteriores apartados.


Al indicar la location hive actualiza la tabla con los nuevos ficheros

Cuando una tabla es externa no se pierden los datos en caso de hacer un drop table

Al hacer load data inpath los datos se mueven del directorio de hdfs al que estaban al de la tabla de la base de datos en la que estamos trabajando.
